fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0, lambda = fit1$lambda.min)
coef(fit2)
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
lam
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response")
hist(pred)
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response")
hist(pred)
confusionMatrix(as.factor(pred > 0.5),ytest)
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response")
hist(pred)
table(as.factor(pred > 0.5),ytest)
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response")
hist(pred)
# table(as.factor(pred > 0.5),ytest)
length(pred)
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response")
hist(pred)
# table(as.factor(pred > 0.5),ytest)
pred
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response", s = fit1$lambda.min)
hist(pred)
# table(as.factor(pred > 0.5),ytest)
pred
table(as.factor(pred > 0.5),ytest)
table(as.factor(pred > 0.3),ytest)
table(as.factor(pred > 0.2),ytest)
table(as.factor(pred > 0.1),ytest)
table(as.factor(pred > 0.1),ytest)
table(as.factor(pred > 0.1),ytest)
(97+10)/(97+1+18+10)
table(as.factor(pred > 0.2),ytest)
(97+10)/(97+1+18+10)
table(as.factor(pred > 0.2),ytest)
(97+10)/(97+1+18+10)
table(as.factor(pred > 0.2),ytest)
(111+4)/(97+1+18+10)
0.8492063
table(as.factor(pred > 0.2),ytest)
(111+4)/(97+1+18+10)
table(as.factor(pred > 0.2),ytest)
acc = (111+4)/(97+1+18+10)
acc
roc2 <- prediction(pred, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
roc2 <- prediction(pred, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit2, s = fit1$lambda.min)
pred = predict(fit2, newx = data.matrix(Xtest), type = "response", s = fit1$lambda.min)
hist(pred)
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
fit2 = glmnet(x = data.matrix(Xtrain),y = ytrain, family = binomial,
alpha = 0)
lam = coef(fit2, s = fit1$lambda.min)
pred = predict(fit1, newx = data.matrix(Xtest), type = "response", s = fit1$lambda.min)
hist(pred)
table(as.factor(pred > 0.5),ytest)
acc = (111+4)/(97+1+18+10)
acc
table(as.factor(pred > 0.5),ytest)
acc = (115+6)/(97+1+18+10)
acc
roc2 <- prediction(pred, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10,
type.measure = "auc", family = "binomial")
pred = predict(fit1, newx = data.matrix(Xtest), type = "response", s = fit1$lambda.min)
hist(pred)
table(as.factor(pred > 0.5),ytest)
acc = (115+6)/length(ytest)
acc
table(as.factor(pred > 0.5),ytest)
acc = (115+6)/length(ytest)
acc
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
roc <- prediction((kmeanfit$cluster - 1), ytest)
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
roc <- prediction((kmeanfit$cluster - 1), sub3$y)
# calculates the ROC curve
perf <- performance(roc2,"tpr","fpr")
plot(perf,colorize=TRUE)
performance(roc, measure = "auc")@y.values[[1]]
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
roc <- prediction((kmeanfit$cluster - 1), sub3$y)
# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)
performance(roc, measure = "auc")@y.values[[1]]
table(as.factor(pred > 0.5),ytest)
acc = (115+6)/length(ytest)
acc
roc2 <- prediction(pred, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
sub3
complete_hc <- hclust(img2,method = "complete")
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
single_hc <- hclust(data1,method = "single")
average_hc <- hclust(data1,method = "average")
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
single_hc <- hclust(data1,method = "single")
average_hc <- hclust(data1,method = "average")
clusters = cutree(average_hc, k = 2)
clusters
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
plot(complete_hc)
single_hc <- hclust(data1,method = "single")
plot(single_hc)
average_hc <- hclust(data1,method = "average")
plot(average_hc)
clusters = cutree(average_hc, k = 2)
clusters
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
plot(complete_hc)
single_hc <- hclust(data1,method = "single")
plot(single_hc)
average_hc <- hclust(data1,method = "average")
plot(average_hc)
clusters = cutree(complete_hc, k = 2)
clusters
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
# plot(complete_hc)
single_hc <- hclust(data1,method = "single")
# plot(single_hc)
average_hc <- hclust(data1,method = "average")
# plot(average_hc)
clusters = cutree(complete_hc, k = 2)
table((clusters - 1),sub3$y)
# sub4 = sub3[,-1099]
# kmeanfit <- kmeans(sub4, 2)
# table((kmeanfit$cluster - 1),sub3$y)
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
# plot(complete_hc)
single_hc <- hclust(data1,method = "single")
# plot(single_hc)
average_hc <- hclust(data1,method = "average")
# plot(average_hc)
clusters = cutree(complete_hc, k = 2)
table((clusters - 1),sub3$y)
roc2 <- prediction((clusters - 1), sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
# sub4 = sub3[,-1099]
# kmeanfit <- kmeans(sub4, 2)
# table((kmeanfit$cluster - 1),sub3$y)
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
# plot(complete_hc)
single_hc <- hclust(data1,method = "single")
# plot(single_hc)
average_hc <- hclust(data1,method = "average")
# plot(average_hc)
clusters = cutree(single_hc, k = 2)
table((clusters - 1),sub3$y)
roc2 <- prediction((clusters - 1), sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
# sub4 = sub3[,-1099]
# kmeanfit <- kmeans(sub4, 2)
# table((kmeanfit$cluster - 1),sub3$y)
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
# plot(complete_hc)
single_hc <- hclust(data1,method = "single")
# plot(single_hc)
average_hc <- hclust(data1,method = "average")
# plot(average_hc)
clusters = cutree(average_hc, k = 2)
table((clusters - 1),sub3$y)
roc2 <- prediction((clusters - 1), sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
# sub4 = sub3[,-1099]
# kmeanfit <- kmeans(sub4, 2)
# table((kmeanfit$cluster - 1),sub3$y)
data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
# plot(complete_hc)
single_hc <- hclust(data1,method = "single")
# plot(single_hc)
average_hc <- hclust(data1,method = "average")
# plot(average_hc)
clusters = cutree(complete_hc, k = 2)
table((clusters - 1),sub3$y)
roc2 <- prediction((clusters - 1), sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
install.packages("caret")
brca = read.csv("brca_data_w_subtypes.csv")
dim(brca) # 705 rows, 1941 columns
names(brca)[1937:1941] # outcomes
# 1936 covariates: 860 copy number variations (cn), 249 somatic mutations (mu), 604 gene expressions (rs), and 223 protein levels (pp)
brca = brca[,-1937] # discard `vital.status`
names(brca)[1937:1940]
hist(brca$rs_CLEC3A, main="Histogram of rs_CLEC3A")
hist(brca$pp_A.Raf, main="Histogram of pp_A.Raf")
barplot(table(brca$cn_A2ML1), main="Bar Plot of cn_A2ML1")
barplot(table(brca$mu_ABCA12), main="Bar Plot of mu_ABCA12")
# only use Negative and Positive for PR.status, ER.status, and HER2.Final.Status
# PR.status and ER.status are highly correlated
table(brca$PR.Status)
table(brca$ER.Status)
table(brca$HER2.Final.Status)
table(brca$histological.type)
# sub is the dataset containing no missing values
sub = brca[(brca$PR.Status == "Positive" | brca$PR.Status == "Negative") &
(brca$ER.Status == "Positive" | brca$ER.Status == "Negative") &
(brca$HER2.Final.Status == "Positive" |
brca$HER2.Final.Status == "Negative"),]
dim(sub)
# the input variables have the indices below
# rs 1:604, cn 605:1464, mu 1465:1713, pp 1714:1936
rs = sub[1:604] # the subset that only contains rs
corr = round(cor(rs), 2) # correlation matrix
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
for (j in 1:nrow(corr)) {
if (abs(corr[i, j]) > 0.8 & i < j) {
idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
}
}
}
idx = idx[-1,] # stores correlations that are greater than 0.8 -> multicollinearity
# dim(idx)
names(idx) = c("i", "j", "corr")
idx[1:3,]
# remove highly-correlated variables
rmv = unique(idx[,1])
length(rmv)
rs = rs[,-rmv]
cn = sub[605:1464]
corr = round(cor(cn), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
for (j in 1:nrow(corr)) {
if (abs(corr[i, j]) > 0.8 & i < j) {
idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
}
}
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")
rmv = unique(idx[,1])
length(rmv)
cn = cn[,-rmv]
mu = sub[1465:1713]
corr = round(cor(mu), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
for (j in 1:nrow(corr)) {
if (abs(corr[i, j]) > 0.8 & i < j) {
idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
}
}
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")
rmv = unique(idx[,1])
length(rmv)
# there is no multicollinearity within mu, so no variable is removed here
# mu = mu[,-rmv]
pp = sub[1714:1936]
corr = round(cor(pp), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
for (j in 1:nrow(corr)) {
if (abs(corr[i, j]) > 0.8 & i < j) {
idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
}
}
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")
rmv = unique(idx[,1])
length(rmv)
pp = pp[,-rmv]
n_outlier = 0
row_idx_rs = c()
col_idx_rs = c()
for (i in 1:ncol(rs)) {
col = rs[,i]
col = qnorm(rank(col) / (1 + length(col))) # normalize
n_outlier = length(col[col < mean(col) - 3 * sd(col) |
col > mean(col) + 3 * sd(col)])
if (n_outlier > 0) {
col_idx_rs = c(col_idx_rs, i)
row_idx_rs = c(row_idx_rs, which(col %in% col[col < mean(col) - 3 * sd(col) |
col > mean(col) + 3 * sd(col)]))
}
}
row_idx_rs = unique(row_idx_rs)
length(row_idx_rs)
length(col_idx_rs)
n_outlier = 0
row_idx_pp = c()
col_idx_pp = c()
for (i in 1:ncol(pp)) {
col = pp[,i]
col = qnorm(rank(col) / (1 + length(col))) # normalize
n_outlier = length(col[col < mean(col) - 3 * sd(col) | col > mean(col) + 3 * sd(col)])
if (n_outlier > 0) {
col_idx_pp = c(col_idx_pp, i)
row_idx_pp = c(row_idx_pp, which(col %in% col[col < mean(col) - 3 * sd(col) |
col > mean(col) + 3 * sd(col)]))
}
}
row_idx_pp = unique(row_idx_pp)
length(row_idx_pp)
length(col_idx_pp)
row_idx = unique(c(row_idx_rs, row_idx_pp))
length(row_idx)
# columns with outliers are skewed (mostly right-skewed)
# for (i in col_idx_rs[1:20]) {
#   hist(rs[,i], main = names(rs)[i])
# }
# outlier example
hist(rs[,col_idx_rs[1]],
main = paste("right-skewed example:", names(rs)[col_idx_rs[1]]))
# no outlier example
hist(rs[,1],
main = paste("right-skewed example (no outlier):", names(rs)[1]))
hist(rs[,9],
main = paste("two mode example (no outlier):", names(rs)[9]))
# columns without outliers
# for (i in 1:16) {
#   hist(rs[,i])
# }
# log transform rs and normalize pp
rs_transformed = rs
pp_normalized = pp
for (i in 1:ncol(rs)) {
rs_transformed[,i] = log(1 + rs[,i])
}
for (i in 1:ncol(pp)) {
pp_normalized[,i] = qnorm(rank(pp[,i]) / (1 + length(pp[,i])))
}
hist(pp[,1],
main = paste("Before Normalization", names(pp)[1]))
hist(pp_normalized[,1],
main = paste("After Normalization", names(pp_normalized)[1]))
par(mfrow=c(2, 2))
barplot(table(sub$PR.Status))
barplot(table(sub$ER.Status))
barplot(table(sub$HER2.Final.Status))
barplot(table(sub$histological.type), names.arg = c("IDC", "ILC"))
for (i in 1:10) {
barplot(table(cn[,i]), main=names(cn)[i], ylim=c(0, 300))
}
# cleaned dataset with PR.status as response
y = as.factor(sub$PR.Status)
sub2 = cbind(rs_transformed, cn, mu, pp_normalized, y)
dim(sub2)
set.seed(651978735)
n = dim(sub)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size
Xtest = sub2[test_idx, -ncol(sub2)]
Xtrain = sub2[-test_idx, -ncol(sub2)]
ytest = sub2[test_idx, ncol(sub2)]
ytrain = sub2[-test_idx, ncol(sub2)]
test_data = sub2[test_idx,]
train_data = sub2[-test_idx,]
# basic fit
library(e1071)
svm.fit = svm(ytrain ~., data=Xtrain,
type="C-classification", kernel="linear", scale=F, cost=1)
table("fitted" = svm.fit$fitted, "actual" = ytrain) # in-sample confusion matrix
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 0.05, 0.1, 0.5, 1))
train_control = trainControl(method="cv", number=5)
svm.linear = train(y ~., data=train_data,
method="svmLinear", trContorl=train_control, tuneGrid=cost.grid)
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 0.05, 0.1, 0.5, 1))
train_control = trainControl(method="cv", number=5)
svm.linear = train(y ~., data=train_data,
method="svmLinear", trContorl=train_control, tuneGrid=cost.grid)
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 0.05, 0.1, 0.5, 1))
train_control = trainControl(method="cv", number=5)
svm.linear = train(y ~., data=train_data,
method="svmLinear", trContorl=train_control, tuneGrid=cost.grid)
