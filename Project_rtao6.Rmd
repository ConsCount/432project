---
title: "STAT 432 Final Project"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

<style>
body {
text-align: justify}
</style>

```{css, echo=FALSE}
.solution {
background-color: #CCDDFF;
}
```

```{r, include=FALSE, warning=FALSE}
# all packages
library(glmnet)
library(e1071)
library(caret)
library(randomForest)
library(ranger)
library(ROCR)
```

# Project Description (est. 1 page, pt. 5)

# Literature Review


The title of our first relevant paper is Tumor characteristics and patient outcomes are similar between invasive lobular and mixed invasive ductal/lobular breast cancers but differ from pure invasive ductal breast cancers. A total of 4,336 individuals with IDC, ILC, and mixed breast tumors were detected between 1996 and 2006. 

The Kaplan-Meier method was used extensively in this paper, and survival curves were constructed using it.Chi-square tests and Fisher's exact tests were used to compare clinical variables.  The correlations between patient and tumor variables were summarized using contingency tables and investigated using Fisher's exact test as among three histologic groups. Patients with ILC and mixed breast cancers were more probable as IDC patients to have tumors that were estrogen receptor and progesterone receptor positive (P < 0.001 and P< 0.05, correspondingly). 

After having read, we can conclude the following from the paper: first, despite being identified at lower clinical stages of infection, patients with IDC had the poorest long-term survival; second, individuals with ILC and "mixed" malignancies had a better prognosis than patients with IDC, despite having more advanced cancer. We were also motivated to utilize the log-rank test to estimate P values if necessary.

The second research article is Infiltrating lobular carcinoma of the breast: tumor characteristics and clinical outcome. We summarize that these patients do not have improved clinical outcomes as IDC patients when ignoring the fact that ILC has a positive biologic pattern. Consequently, management decisions should be made based on the patient's and tumor's biologic characteristics, that instead of lobular histology. 

About statistical methods, the clinical and biologic features of lobular and ductal carcinoma were compared by contingency tables, Chi-square tests and Fisher’s exact tests, which is similar with the method using in the first paper. To see if ILC was an independent predictive predictor for recurrence and death, researchers used multivariate analysis and Cox regression models. Tumor size, number of affected nodes, age, ER status, PgR status, DNA ploidy, S-phase, and histologic type were all considered in these analyses.

The findings of this huge dataset have shown that ILC and IDC are distinct entities with distinctive clinical histories and biologic features, yet there are no clinically important variations in survival. At the present, both kinds of breast cancer should be treated identically, and histologic subtype (lobular or ductal) should not be regarded a determinant in therapeutic decision-making or an essential prognostic or predictive factor at diagnosis. Emerging technologies such as high throughput genome mapping and microchip cDNA expression arrays may help to uncover molecular distinctions between these different types of breast cancer.


# Summary Statistics and Data Preprocessing

## Data Overview  
The dataset has 705 observations and 1941 features (1936 predictors and 5 outcomes). There are four different kinds of predictors: `rs` (gene expression), `cn` (copy number variations), `mu` (mutations), and `pp` (protein levels). Among them, `rs` and `pp` are continuous variables, and `cn` and `mu` are categorical variables. 

```{r load data, include=FALSE}
brca = read.csv("brca_data_w_subtypes.csv")
```

```{r, include=FALSE}
dim(brca) # 705 rows, 1941 columns
names(brca)[1937:1941] # outcomes 
# 1936 covariates: 860 copy number variations (cn), 249 somatic mutations (mu), 604 gene expressions (rs), and 223 protein levels (pp)
```

```{r, include=FALSE}
brca = brca[,-1937] # discard `vital.status` 
names(brca)[1937:1940]
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="23%"}
hist(brca$rs_CLEC3A, main="Histogram of rs_CLEC3A")
hist(brca$pp_A.Raf, main="Histogram of pp_A.Raf")
barplot(table(brca$cn_A2ML1), main="Bar Plot of cn_A2ML1")
barplot(table(brca$mu_ABCA12), main="Bar Plot of mu_ABCA12")
```

## Remove Missing Values
According to the instruction, we dropped `vital.status`, and we only considered each response variable as a binary variable. Therefore, we treated the observations that had other outcomes as missing values and removed them from our dataset. 

```{r, include=FALSE}
# only use Negative and Positive for PR.status, ER.status, and HER2.Final.Status
# PR.status and ER.status are highly correlated
table(brca$PR.Status) 
table(brca$ER.Status)
table(brca$HER2.Final.Status)
table(brca$histological.type)
```

Then the dataset `sub` had 507 observations and 1940 features. 
```{r, include=FALSE}
# sub is the dataset containing no missing values 
sub = brca[(brca$PR.Status == "Positive" | brca$PR.Status == "Negative") & 
           (brca$ER.Status == "Positive" | brca$ER.Status == "Negative") & 
           (brca$HER2.Final.Status == "Positive" | 
            brca$HER2.Final.Status == "Negative"),]
```

```{r}
dim(sub)
```

```{r, include=FALSE}
# the input variables have the indices below 
# rs 1:604, cn 605:1464, mu 1465:1713, pp 1714:1936
```

## Deal with Multicollinearity 
One of the noticeable characteristics of the data is its high dimensionality. There are 1936 predictors, almost four times as many as there are observations. Therefore, it is essential to check correlation.   

Since there are four kinds of predictors, it is unlikely that two variables coming from different kinds would be highly correlated. Also, to reduce the computational cost, we split the data into four subsets: `rs`, `cn`, `mu`, and `pp`, each of which contained only ond kind of predictors.  

Then, we created the correlation matrix for each subset, and extracted variables that are highly-correlated with at least one other variable. Take `rs` as an example. The dataframe `idx` stores all matrix indices of highly-correlated variables and the corresponding correlation coefficients. If the i-th variable is highly-correlated with the j-th variable, then we only need one of them. Thus, we removed all variables with indices `i`. For `rs`, 94 predictors were removed. We applied the same process to the other three subsets. In total, 882 predictors were removed. There are 1059 predictors remained. 

```{r, include=FALSE}
rs = sub[1:604] # the subset that only contains rs
corr = round(cor(rs), 2) # correlation matrix 
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,] # stores correlations that are greater than 0.8 -> multicollinearity 
# dim(idx)
```

```{r}
names(idx) = c("i", "j", "corr")
idx[1:3,]
# remove highly-correlated variables 
rmv = unique(idx[,1])
length(rmv)
rs = rs[,-rmv]
```

```{r, include=FALSE}
cn = sub[605:1464]
corr = round(cor(cn), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)

cn = cn[,-rmv]
```

```{r, include=FALSE}
mu = sub[1465:1713]
corr = round(cor(mu), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)
# there is no multicollinearity within mu, so no variable is removed here 
# mu = mu[,-rmv]
```

```{r, include=FALSE}
pp = sub[1714:1936]
corr = round(cor(pp), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)

pp = pp[,-rmv]
```

## Continuous Variables  
As mentioned before, `rs` and `pp` are continuous variables, so we should examine if there are any outliers. We first normalized the variable, and stored row and column indices if the data point was three standard deviations away from the mean. For the two subsets, `rs` had 100 outliers, and `pp` had no outlier.   

We further looked into `rs` predictors that included outliers, and we found the vast majority of them had a long tail, mostly right and some left. In addition, a number of `rs` predictors that did not contain outliers also had a non-standard distribution. As a result, a log transformation of `rs` predictors would be beneficial.  

```{r, include=FALSE}
n_outlier = 0
row_idx_rs = c()
col_idx_rs = c()
for (i in 1:ncol(rs)) {
  col = rs[,i]
  col = qnorm(rank(col) / (1 + length(col))) # normalize 
  n_outlier = length(col[col < mean(col) - 3 * sd(col) | 
                           col > mean(col) + 3 * sd(col)])
  if (n_outlier > 0) {
    col_idx_rs = c(col_idx_rs, i)
    row_idx_rs = c(row_idx_rs, which(col %in% col[col < mean(col) - 3 * sd(col) | 
                                              col > mean(col) + 3 * sd(col)]))
  }
}
row_idx_rs = unique(row_idx_rs)
length(row_idx_rs)
length(col_idx_rs)
```

```{r, include=FALSE}
n_outlier = 0
row_idx_pp = c()
col_idx_pp = c()

for (i in 1:ncol(pp)) {
  col = pp[,i]
  col = qnorm(rank(col) / (1 + length(col))) # normalize 
  n_outlier = length(col[col < mean(col) - 3 * sd(col) | col > mean(col) + 3 * sd(col)])
  if (n_outlier > 0) {
    col_idx_pp = c(col_idx_pp, i)
    row_idx_pp = c(row_idx_pp, which(col %in% col[col < mean(col) - 3 * sd(col) | 
                                              col > mean(col) + 3 * sd(col)]))
  }
}

row_idx_pp = unique(row_idx_pp)
length(row_idx_pp)
length(col_idx_pp)
```

```{r, include=FALSE}
row_idx = unique(c(row_idx_rs, row_idx_pp))
length(row_idx)
```

```{r, include=FALSE}
# columns with outliers are skewed (mostly right-skewed)
# for (i in col_idx_rs[1:20]) {
#   hist(rs[,i], main = names(rs)[i])
# }
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="30%"}
# outlier example 
hist(rs[,col_idx_rs[1]], 
     main = paste("right-skewed example (had outliers):", names(rs)[col_idx_rs[1]]))

# no outlier example 
hist(rs[,1], 
     main = paste("right-skewed example (no outlier):", names(rs)[1]))
hist(rs[,9], 
     main = paste("two mode example (no outlier):", names(rs)[9]))
```

```{r, include=FALSE}
# columns without outliers 
# for (i in 1:16) {
#   hist(rs[,i])
# }
```

Unlike `rs`, `pp` variables were distributed quite normally. However, many of the variables would contain outliers without normalization. Therefore, we normalized `pp` variables. 

```{r, include=FALSE}
# log transform rs and normalize pp 
rs_transformed = rs
pp_normalized = pp
for (i in 1:ncol(rs)) {
  rs_transformed[,i] = log(1 + rs[,i])
}

for (i in 1:ncol(pp)) {
  pp_normalized[,i] = qnorm(rank(pp[,i]) / (1 + length(pp[,i])))
}
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="30%"}
hist(pp[,1], 
     main = paste("Before Normalization:", names(pp)[1]))
hist(pp_normalized[,1], 
     main = paste("After Normalization:", names(pp_normalized)[1]))
```

## Categorical Variables  
All four outcomes were more or less imbalanced, among which `histological.type` was the most imbalanced. Only 10% of the responses were ILC. In some cases, such imbalance would be problematic since models would learn nothing from the minority class. If our data also suffer from problems like this, we should resolve the imbalance by techniques like undersampling. However, if our models perform well enough on current data, no further action needs to be taken. 

Fortunately, we trained our models first, and found the models obtained high enough accuracy and AUC scores. Therefore, we decided to not address imbalanced outcomes. 

```{r, echo=FALSE}
table(sub$PR.Status) / nrow(sub)
table(sub$ER.Status) / nrow(sub)
table(sub$histological.type) / nrow(sub)
table(sub$HER2.Final.Status) / nrow(sub)
```


```{r, echo=FALSE, out.width="30%", fig.show="hold", fig.align="center"}
barplot(table(sub$histological.type),
        ylim=c(0, 500),
        main="Histological Type",
        names.arg = c("IDC", "ILC"))
```

# Modeling `PR Status`

Before modeling, we split the train and test datasets. We used 25% of the samples (126) for testing and 75% of the samples (381) for training. 

```{r, include=FALSE}
# cleaned dataset with PR.status as response 
y = as.factor(sub$PR.Status)
sub2 = cbind(rs_transformed, cn, mu, pp_normalized, y)
dim(sub2)
```

```{r, include=FALSE}
set.seed(651978735) 
n = dim(sub)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size 

Xtest = sub2[test_idx, -ncol(sub2)]
Xtrain = sub2[-test_idx, -ncol(sub2)]

ytest = sub2[test_idx, ncol(sub2)]
ytrain = sub2[-test_idx, ncol(sub2)]

test_data = sub2[test_idx,]
train_data = sub2[-test_idx,]
```

## Support Vector Machine (SVM)

The goal of the project was to make classifications. Plus, we needed to alleviate "the curse of dimensionality". Therefore, we should choose classification models that perform well on high-dimensional data. Support vector machines are famous for its capability in high-dimensional spaces, so we first fitted a basic linear SVM, with the default `cost = 1` to see how it worked.

As the confusion matrix showed, the in-sample accuracy was 1.0, which implied that we might prefer the linear kernel to the radial kernel.
```{r basic linear fit, echo=FALSE}
<<<<<<< HEAD
# basic fit
library(e1071)
svm.fit = svm(ytrain ~., data=Xtrain,
=======
# basic fit 
svm.fit = svm(ytrain ~., data=Xtrain, 
>>>>>>> 3061f9a33c978d6e05c95f2d98427e59974ad95c
              type="C-classification", kernel="linear", scale=F, cost=1)
table("predicted" = svm.fit$fitted, "actual" = ytrain) # in-sample confusion matrix
```

Then we constructed two grids of tuning parameters for both linear and radial kernels, and we used 5-fold cross-validation to tune the parameters and to determine which kernel was better.

For the linear kernel, the best `C` was 0.001 with an in-sample accuracy of 0.8324.

```{r, warning=FALSE, include=FALSE}
<<<<<<< HEAD
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 0.05, 0.1, 0.5, 1))
=======
cost.grid = expand.grid(C = c(0.001, 0.01, 0.05, 0.1, 0.5, 1)) 
>>>>>>> 3061f9a33c978d6e05c95f2d98427e59974ad95c
train_control = trainControl(method="cv", number=5)
svm.linear = train(y ~., data=train_data,
                   method="svmLinear", trContorl=train_control, tuneGrid=cost.grid)
```

```{r, echo=FALSE}
svm.linear$bestTune
svm.linear$results
```

For the radial kernel, the best `C` was 0.001 and the best `sigma` was 3. However, the figure showed that the radial SVM fitted poorly, since the accuracies remained the same and were merely 0.6677. The fact verified our hypothesis that a linear kernel would work better. Thus, we picked the linear SVM with `C` equals 0.001 to make classifications.

```{r SVM Radial Kernel, warning=FALSE, include=FALSE}
<<<<<<< HEAD
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 1),
                        sigma = c(0.1, 1, 3))
=======
cost.grid = expand.grid(C = c(0.001, 0.01, 1), 
                        sigma = c(0.1, 1, 3)) 
>>>>>>> 3061f9a33c978d6e05c95f2d98427e59974ad95c
train_control = trainControl(method="cv", number=5)
svm.radial = train(y ~., data=train_data,
                   method="svmRadial", trContorl=train_control, tuneGrid=cost.grid)
```

```{r, echo=FALSE}
svm.radial$bestTune
# svm.radial$results
```

```{r, echo=FALSE, fig.align="center", out.width="60%"}
plot(svm.radial)
```

<<<<<<< HEAD

We made predictions for the test data, and printed the confusion table below. The accuracy was 0.9048. Thus, the linear SVM performed quite well.
=======
We made predictions for the test data, and printed the confusion table below. The accuracy was 0.9048. Thus, the linear SVM performed quite well. 
>>>>>>> 3061f9a33c978d6e05c95f2d98427e59974ad95c
```{r, echo=FALSE}
pred = predict(svm.linear, newdata = Xtest)
confusion_table = table("predicted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

## Random Forest
Random forests are another classification model that is less vulnerable to "the curse of dimensionality". Since there were many parameters that needed to be tuned, we again utilized `caret` package to cross validate the model.

Before applying cross-validation, we should first determine `num.trees`, because it could not be included in the grid of parameters. Therefore, we first fitted a random forest using the `randomForest()` method with `ntree = 1000`, and plotted the error against trees.

```{r, include=FALSE}
set.seed(651978735)
rf.fit = randomForest(Xtrain, ytrain,
                      ntree=1000,
                      mtry=20,
                      nodesize=5,
                      samplesize=400,
                      importance=TRUE)
```

As the plot demonstrated, the error stopped decreasing after the number of trees reached around 500. Thus, `ntree = 500` should be sufficient for training.
```{r, echo=FALSE, fig.align="center", out.width="80%"}
plot(rf.fit,
     main="Random Forest ntree Selection")
```

In a 5-fold cross-validation, we tuned `mtry` and `min.node.size`. The output showed that the best parameters were `mtry = 40` and `min.node.size = 15` when `num.trees = 500` according to the test accuracy.

```{r, include=FALSE}
set.seed(651978735)
grid = expand.grid(mtry=c(20, 32, 40),
                   splitrule="gini",
                   min.node.size=c(5, 10, 15))
train_control = trainControl(method="cv", number=5)

rf.cv.fit = train(y ~., data=train_data,
               method="ranger",
               trControl=train_control,
               tuneGrid=grid,
               num.trees=500,
               respect.unordered.factors="partition")
```

```{r, echo=FALSE}
rf.cv.fit$bestTune
rf.cv.fit$results
```

```{r, echo=FALSE, fig.align="center", out.width="80%"}
plot(rf.cv.fit, xlab="Number of Randomly Selected Predictors")
```

The confusion matrix and the highest test accuracy, 0.9048, were shown here.
```{r, echo=FALSE}
pred = predict(rf.cv.fit, Xtest)
confusion_table = table("predicted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

# Modeling `Histological Type`

To establish the Modeling for histological Type, We observe that the response variable `Histological Type` has only two levels “infiltrating lobular carcinoma” and “infiltrating ductal carcinoma”. Thus, Logistic model become a good choice. To Peform a logistic Model, we first split the data into train and test data. Test data will be random selected 25% of the sample size. 


```{r some-processing, include= FALSE}
y = as.factor(sub$histological.type)
y = as.factor(ifelse(y == "infiltrating lobular carcinoma", 1, 0))
sub3 = cbind(rs, cn, mu, pp, y) # cleaned dataset with PR.status as response 
```

```{r split-train-test-data, include= FALSE}
set.seed(651978735) 
n = dim(sub3)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size 

Xtest = sub3[test_idx, -ncol(sub3)]
Xtrain = sub3[-test_idx, -ncol(sub3)]
train = sub3[-test_idx]
ytest = sub3[test_idx, ncol(sub3)]
ytrain = sub3[-test_idx, ncol(sub3)]
```
## Logistic Regression

Once, we attempted to fit logistic model with our data, the algorithm throw a warning said "algorithm did not converge". In other words, we are experience a perfect sepration. As the confusion matrix shows below, our model has 100% accuracy and AUC with 1. 
```{r, echo= FALSE}
library(ROCR)
logistic.fit <- glm(y~., data = sub3, family = binomial)
roc2 <- prediction(logistic.fit$fitted.values, sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
# plot(perf2,colorize=TRUE)
cat("AUC =", performance(roc2, measure = "auc")@y.values[[1]])
table(logistic.fit$fitted.values > 0.5, sub3$y)
```


<<<<<<< HEAD

## Logistic Regression with ridge penalty and corss validation.


The above model is obviously not correct. Thus, we decided to use penalized regression and cross validation. Thus, we are going to perform a logistic model with ridge penalty and 10-fold cross validation. Since, the Logistic Regresion Model will not return value with 0 and 1 Thus, we decided to choose the cut-off value based on the distribution of the predict result. Thus, we plot the histogram of the prediction result. 

```{r logistic regression with ridge penalty and 10 fold corss validation, echo = FALSE, message=FALSE ,fig.align="center", out.width="80%"}
library(glmnet)
=======
```{r logistic regression with ridge penalty and 10 fold corss validation}
>>>>>>> 3061f9a33c978d6e05c95f2d98427e59974ad95c
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10, 
                 type.measure = "auc", family = "binomial")
pred = predict(fit1, newx = data.matrix(Xtest), type = "response", s = fit1$lambda.min)
hist(pred)
```

Based on the graph shows above, we decided to choose cut-off value as 0.5. The the prediction result will give 121 FALSE and 5 TRUE.

```{r, echo= FALSE }
table(pred > 0.5)
```
Then, we are going to plot the ROC and calculate AUC. 

<<<<<<< HEAD
```{r calculate auc, echo=FALSE,fig.align="center", out.width="80%"}
=======
```{r calculate auc}
>>>>>>> 3061f9a33c978d6e05c95f2d98427e59974ad95c
roc2 <- prediction(pred, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
cat("AUC =",performance(roc2, measure = "auc")@y.values[[1]])
table(pred > 0.5, ytest)
```

Based on the ROC and output given above, AUC is 0.915415, which is a good result 

## kmean clustering 

```{r perform kmean}
# sub4 = sub3[,-1099]
# kmeanfit <- kmeans(sub4, 2)
# table((kmeanfit$cluster - 1),sub3$y)
pca <- prcomp(sub3[, -ncol(sub3)])

plot(pca, type = "l")

comp = pca$x[,1:4]
kfit = kmeans(comp,2)
clusters = kfit$cluster - 1
table(clusters,sub3$y)

roc2 <- prediction(clusters, sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
```

## tunning tree 

```{r}
library(rpart)
fit = rpart(as.factor(y)~., data= train,control = rpart.control(xval = 10))
fit$cptable
# prunedtree = prune(fit, cp=cptarg)
# rpart.plot(prunedtree)
# table(,ytest)
pred = predict(fit,Xtest)
result = ifelse(pred[,1] > pred[,2], 0, 1)
table(result,ytest)

roc2 <- prediction(result, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
```

# Variable Selection for All Outcomes (random forest?) (est. 2-3 pages. pt.20)

Using Random Forest, select the most important 50 variables, and make predictions based on these variables.

**NOTE: have not cross validated, have not fitted for all outcomes (PR.Status, ER.Status, histological.type, HER2.Final.Status)**

```{r}
# fit a random forest with best parameters selected by cross-validation
set.seed(651978735)
rf.fit = randomForest(Xtrain, ytrain,
                      ntree=500,
                      mtry=40,
                      nodesize=15,
                      samplesize=400,
                      importance=TRUE)
```

```{r}
# check the model's test accuracy
pred = predict(rf.fit, Xtest)
confusion_table = table("predicted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

```{r}
# selected the most important 50 variables
impt = rf.fit$importance[order(rf.fit$importance[,3], decreasing=TRUE),][1:50,]
vars = c(rownames(impt), c("pp_E.Cadherin", "pp_PTEN", "mu_TBX3", "cn_FOXA1", "mu_FOXA1", "rs_FOXA1"))
```

```{r}
# sub4 is the cleaned dataset with all four response variables
# columns: 50 predictors, 4 outcomes
sub4 = subset(sub, select = vars)
sub4 = cbind(sub4, sub[1937:1940])
sub4$PR.Status = as.factor(sub4$PR.Status)
sub4$histological.type = as.factor(sub4$histological.type)
sub4$ER.Status = as.factor(sub4$ER.Status)
sub4$HER2.Final.Status = as.factor(sub4$HER2.Final.Status)
dim(sub4)
```

```{r}
# split test and train
Xtest = sub4[test_idx, 1:50]
Xtrain = sub4[-test_idx, 1:50]
```

```{r}
# fit for ER.Status (SVM)
ytest = sub4$ER.Status[test_idx]
ytrain = sub4$ER.Status[-test_idx]
svm.fit = svm(ytrain ~., data=Xtrain,
              type="C-classification", kernel="linear", scale=F, cost=1)
table("predicted" = svm.fit$fitted, "actual" = ytrain)
pred = predict(svm.fit, newdata = Xtest)
table("predicted" = pred, "actual" = ytest)
```

```{r}
# the AUC is 0.84
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr")
plot(perf, colorize = T)
```

```{r}
# fit for ER.Status, random forest, AUC is 0.90
rf.fit = randomForest(Xtrain, ytrain, 
                      ntree=500, 
                      mtry=40, 
                      nodesize=15, 
                      samplesize=400, 
                      importance=TRUE)

pred = predict(rf.fit, Xtest)
table("predicted" = pred, "actual" = ytest)
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr")
plot(perf, colorize = T)
```

```{r}
set.seed(1)
fold_ID = sample(1:3, 705, replace = TRUE)
fold_id = fold_ID[1:nrow(sub4)]
```
