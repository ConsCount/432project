---
title: "Project"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

<style>
body {
text-align: justify}
</style>

```{css, echo=FALSE}
.solution {
background-color: #CCDDFF;
}
```

**Note: have not finished data preprocessing**   

# Project Description (est. 1 page, pt. 5)

# Literature Review(est. 1 page, pt.10)

# Summary Statistics and Data Preprocessing (est. 1 -2 pages, pt.10)

## Data Overview  
The dataset has 705 observations and 1941 features (1936 predictors and 5 outcomes). There are four different kinds of predictors: `rs` (gene expression), `cn` (copy number variations), `mu` (mutations), and `pp` (protein levels). Among them, `rs` and `pp` are continuous variables, and `cn` and `mu` are categorical variables. 

```{r load data, include=FALSE}
brca = read.csv("brca_data_w_subtypes.csv")
```

```{r, include=FALSE}
dim(brca) # 705 rows, 1941 columns
names(brca)[1937:1941] # outcomes 
# 1936 covariates: 860 copy number variations (cn), 249 somatic mutations (mu), 604 gene expressions (rs), and 223 protein levels (pp)
```

```{r, include=FALSE}
brca = brca[,-1937] # discard `vital.status` 
names(brca)[1937:1940]
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="23%"}
hist(brca$rs_CLEC3A, main="Histogram of rs_CLEC3A")
hist(brca$pp_A.Raf, main="Histogram of pp_A.Raf")
barplot(table(brca$cn_A2ML1), main="Bar Plot of cn_A2ML1")
barplot(table(brca$mu_ABCA12), main="Bar Plot of mu_ABCA12")
```

## Remove Missing Values
According to the instruction, we dropped `vital.status`, and we only considered each response variable as a binary variable. Therefore, we treated the observations that had other outcomes as missing values and removed them from our dataset. 

```{r, include=FALSE}
# only use Negative and Positive for PR.status, ER.status, and HER2.Final.Status
# PR.status and ER.status are highly correlated
table(brca$PR.Status) 
table(brca$ER.Status)
table(brca$HER2.Final.Status)
table(brca$histological.type)
```

Then the dataset `sub` had 507 observations and 1940 features. 
```{r}
# sub is the dataset containing no missing values 
sub = brca[(brca$PR.Status == "Positive" | brca$PR.Status == "Negative") & 
           (brca$ER.Status == "Positive" | brca$ER.Status == "Negative") & 
           (brca$HER2.Final.Status == "Positive" | 
            brca$HER2.Final.Status == "Negative"),]
dim(sub)
```

```{r, include=FALSE}
# the input variables have the indices below 
# rs 1:604, cn 605:1464, mu 1465:1713, pp 1714:1936
```

## Deal with Multicollinearity 
One of the noticeable characteristics of the data is its high dimensionality. There are 1936 predictors, almost four times as many as there are observations. Therefore, it is essential to check correlation.   

Since there are four kinds of predictors, it is unlikely that two variables that belong to different kinds would be highly correlated. Also, to reduce the computational cost, we split the data into four subsets: `rs`, `cn`, `mu`, and `pp`, each of which contained only ond kind of predictors.  

Then, we created the correlation matrix for each subset, and extracted variables that are highly-correlated with at least one other variable. Take `rs` as an example. The dataframe `idx` stores all matrix indices of highly-correlated variables and the corresponding correlation coefficients. If the i-th variable is highly-correlated with the j-th variable, then we only need one of them. Thus, we removed all variables with indices `i`. For `rs`, 94 predictors were removed. We applied the same process to the other three subsets. In total, 882 predictors were removed. There are 1059 predictors remained. 

```{r, include=FALSE}
rs = sub[1:604] # the subset that only contains rs
corr = round(cor(rs), 2) # correlation matrix 
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,] # stores correlations that are greater than 0.8 -> multicollinearity 
# dim(idx)
```

```{r}
names(idx) = c("i", "j", "corr")
idx[1:3,]
# remove highly-correlated variables 
rmv = unique(idx[,1])
length(rmv)
rs = rs[,-rmv]
```

```{r, include=FALSE}
cn = sub[605:1464]
corr = round(cor(cn), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)

cn = cn[,-rmv]
```

```{r, include=FALSE}
mu = sub[1465:1713]
corr = round(cor(mu), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)
# there is no multicollinearity within mu, so no variable is removed here 
# mu = mu[,-rmv]
```

```{r, include=FALSE}
pp = sub[1714:1936]
corr = round(cor(pp), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)

pp = pp[,-rmv]
```

## Continuous Variables  
As mentioned before, `rs` and `pp` are continuous variables, so we should examine if there are any outliers. We first normalized the variable, and stored row and column indices if the data point was three standard deviations away from the variable mean. For the two subsets, `rs` had 100 outliers, and `pp` had no outlier.   

We further looked into `rs` predictors that included outliers, and we found the vast majority of them had a long tail, mostly right and some left. In addition, a number of `rs` predictors that did not contain outliers also had a non-standard distribution. As a result, a log transformation of `rs` predictors would be beneficial.  

```{r, include=FALSE}
n_outlier = 0
row_idx_rs = c()
col_idx_rs = c()
for (i in 1:ncol(rs)) {
  col = rs[,i]
  col = qnorm(rank(col) / (1 + length(col))) # normalize 
  n_outlier = length(col[col < mean(col) - 3 * sd(col) | 
                           col > mean(col) + 3 * sd(col)])
  if (n_outlier > 0) {
    col_idx_rs = c(col_idx_rs, i)
    row_idx_rs = c(row_idx_rs, which(col %in% col[col < mean(col) - 3 * sd(col) | 
                                              col > mean(col) + 3 * sd(col)]))
  }
}
row_idx_rs = unique(row_idx_rs)
length(row_idx_rs)
length(col_idx_rs)
```

```{r, include=FALSE}
n_outlier = 0
row_idx_pp = c()
col_idx_pp = c()

for (i in 1:ncol(pp)) {
  col = pp[,i]
  col = qnorm(rank(col) / (1 + length(col))) # normalize 
  n_outlier = length(col[col < mean(col) - 3 * sd(col) | col > mean(col) + 3 * sd(col)])
  if (n_outlier > 0) {
    col_idx_pp = c(col_idx_pp, i)
    row_idx_pp = c(row_idx_pp, which(col %in% col[col < mean(col) - 3 * sd(col) | 
                                              col > mean(col) + 3 * sd(col)]))
  }
}

row_idx_pp = unique(row_idx_pp)
length(row_idx_pp)
length(col_idx_pp)
```

```{r, include=FALSE}
row_idx = unique(c(row_idx_rs, row_idx_pp))
length(row_idx)
```

```{r, include=FALSE}
# columns with outliers are skewed (mostly right-skewed)
# for (i in col_idx_rs[1:20]) {
#   hist(rs[,i], main = names(rs)[i])
# }
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="30%"}
# outlier example 
hist(rs[,col_idx_rs[1]], 
     main = paste("right-skewed example:", names(rs)[col_idx_rs[1]]))

# no outlier example 
hist(rs[,1], 
     main = paste("right-skewed example (no outlier):", names(rs)[1]))
hist(rs[,9], 
     main = paste("two mode example (no outlier):", names(rs)[9]))
```

```{r, include=FALSE}
# columns without outliers 
# for (i in 1:16) {
#   hist(rs[,i])
# }
```

Unlike `rs`, `pp` variables were distributed quite normally. However, many of the variables would contain outliers without normalization. Therefore, we normalized `pp` variables. 

```{r, include=FALSE}
# log transform rs and normalize pp 
rs_transformed = rs
pp_normalized = pp
for (i in 1:ncol(rs)) {
  rs_transformed[,i] = log(1 + rs[,i])
}

for (i in 1:ncol(pp)) {
  pp_normalized[,i] = qnorm(rank(pp[,i]) / (1 + length(pp[,i])))
}
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="40%"}
hist(pp[,1], 
     main = paste("Before Normalization", names(pp)[1]))
hist(pp_normalized[,1], 
     main = paste("After Normalization", names(pp_normalized)[1]))
```

## Categorical Variables  
```{r}
par(mfrow=c(2, 2))
barplot(table(sub$PR.Status))
barplot(table(sub$ER.Status))
barplot(table(sub$HER2.Final.Status))
barplot(table(sub$histological.type), names.arg = c("IDC", "ILC"))
```

```{r, include=FALSE}
for (i in 1:10) {
  barplot(table(cn[,i]), main=names(cn)[i], ylim=c(0, 300))
}
```

# Modeling `PR Status` (est. 2-3 pages, pt.20)

Before modeling, we split the train and test datasets. We used 25% of the samples (126) for testing and 75% of the samples for trianing (381). 

```{r, include=FALSE}
# cleaned dataset with PR.status as response 
y = as.factor(sub$PR.Status)
sub2 = cbind(rs_transformed, cn, mu, pp_normalized, y)
dim(sub2)
```

```{r, include=FALSE}
set.seed(651978735) 
n = dim(sub)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size 

Xtest = sub2[test_idx, -ncol(sub2)]
Xtrain = sub2[-test_idx, -ncol(sub2)]

ytest = sub2[test_idx, ncol(sub2)]
ytrain = sub2[-test_idx, ncol(sub2)]

test_data = sub2[test_idx,]
train_data = sub2[-test_idx,]
```

## Support Vector Machine (SVM)

The goal of the project was to make classifications. Plus, we needed to deal with high-dimensional data. Therefore, we should choose classification models that perform well for high-dimensional data. Support vector machines are famous for its capability in high-dimensional spaces, so we first fitted a basic linear SVM, with the default `cost = 1` to see how it worked.   

As the confusion matrix showed, the in-sample accuracy was 1.0, which implies that we may prefer the linear kernel to the radial kernel. 

```{r basic linear fit, echo=FALSE}
# basic fit 
library(e1071)
svm.fit = svm(ytrain ~., data=Xtrain, 
              type="C-classification", kernel="linear", scale=F, cost=1)
table("predicted" = svm.fit$fitted, "actual" = ytrain) # in-sample confusion matrix 
```

Then we constructed two grids of tuning parameters for both linear and radial kernels, and we used 5-fold cross-validation to tune the parameters and to determine which kernel was better.  

For the linear kernel, the best `C` was 0.001 with an in-sample accuracy of 0.8324. 

```{r, warning=FALSE, include=FALSE}
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 0.05, 0.1, 0.5, 1)) 
train_control = trainControl(method="cv", number=5)
svm.linear = train(y ~., data=train_data,
                   method="svmLinear", trContorl=train_control, tuneGrid=cost.grid)
```

```{r, echo=FALSE}
svm.linear$bestTune
svm.linear$results
```

For the radial kernel, the best `C` was 0.001 and the best `sigma` was 3. However, the results showed that the radial SVM fitted poorly, since the accuracies remained the same and were merely 0.6677. The fact verified our hypothesis that a linear kernel would work better. Thus, we picked the linear SVM with `C` equals 0.001 to make classifications. 

```{r SVM Radial Kernel, warning=FALSE, include=FALSE}
library(caret)
cost.grid = expand.grid(C = c(0.001, 0.01, 1), 
                        sigma = c(0.1, 1, 3)) 
train_control = trainControl(method="cv", number=5)
svm.radial = train(y ~., data=train_data,
                   method="svmRadial", trContorl=train_control, tuneGrid=cost.grid)
```

```{r, echo=FALSE}
svm.radial$bestTune
svm.radial$results
```

We made predictions for the test data, and printed the confusion table below. The accuracy was 0.9048. Thus, the linear SVM performed quite well. 
```{r, echo=FALSE}
pred = predict(svm.linear, newdata = Xtest)
confusion_table = table("predicted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

```{r, include=FALSE}
# library(ROCR)
# roc = prediction(as.numeric(pred), ytest)
# performance(roc, measure = "auc")@y.values[[1]]
# 
# perf = performance(roc, "tpr", "fpr") 
# plot(perf, colorize = T)
```


## Random Forest   
Random forests are another classification model that is less vulnerable to "the curse of dimensionality". Since there were many parameters that needed to be tuned, we again utilized `caret` package to cross validate the model. 

```{r, include=FALSE}
library(ranger)
set.seed(651978735)
grid = expand.grid(mtry=c(20, 32, 40), 
                   splitrule="gini", 
                   min.node.size=c(5, 10, 15))
train_control = trainControl(method="cv", number=5)

rf.cv.fit = train(y ~., data=train_data,
               method="ranger",
               trControl=train_control, 
               tuneGrid=grid,
               num.trees=1000,
               respect.unordered.factors="partition")
```

In a 5-fold cross-validation, we tuned `mtry` and `min.node.size`. In addition, we found that `num.trees` was also influential to both the parameter tuning and the test accuracy. Thus, we manually tuned the number of trees and chose the best value by looking at the test accuracy. The output showed that the best parameters were `mtry = 40` and `min.node.size = 15` when `num.trees = 1000` according to the test accuracy.   

```{r, echo=FALSE}
rf.cv.fit$bestTune
rf.cv.fit$results
```

The confusion matrix and the highest test accuracy, 0.9048, were shown here. 

```{r, echo=FALSE}
pred = predict(rf.cv.fit, Xtest)
confusion_table = table("predicted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

```{r, eval=FALSE, include=FALSE}
# library(ROCR)
# roc = prediction(as.numeric(pred), ytest)
# performance(roc, measure = "auc")@y.values[[1]]
# 
# perf = performance(roc, "tpr", "fpr") 
# plot(perf, colorize = T)
```


# Histological Type (hcluster and knn regression) (est 2-3 pages, pt.20)
To establish the Modeling for histological,  We first use logistical regression, However, when we build the model, we found that the logistic model's algothrim is not coverge. The reason that this error occur, because the variable x can divide the reponse variable y into 0 and 1 perfectly. The accurate will be 100%. To solve the problem we decided to use penalized regression. Thus, we choose the modle of logistic regession with ridge penalty



## Logistic Regression
To performance 

```{r some-processing}
y = as.factor(sub$histological.type)

y = as.factor(ifelse(y == "infiltrating lobular carcinoma", 1, 0))
sub3 = cbind(rs, cn, mu, pp, y) # cleaned dataset with PR.status as response 
```

```{r split-train-test-data}
set.seed(651978735) 
n = dim(sub)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size 

Xtest = sub3[test_idx, -ncol(sub2)]
Xtrain = sub3[-test_idx, -ncol(sub2)]

ytest = sub3[test_idx, ncol(sub2)]
ytrain = sub3[-test_idx, ncol(sub2)]
```

## logistic regression with ridge penalty and 10 fold corss validation.

```{r logistic regression with ridge penalty and 10 fold corss validation}
library(glmnet)
fit1 = cv.glmnet(x = data.matrix(Xtrain), y = ytrain, nfolds = 10, 
                 type.measure = "auc", family = "binomial")
pred = predict(fit1, newx = data.matrix(Xtest), type = "response", s = fit1$lambda.min)
hist(pred)
```

```{r accurate, include = FALSE}
table(as.factor(pred > 0.5),ytest)
acc = (115+6)/length(ytest)
acc
```

```{r calculate auc}
library(ROCR)
roc2 <- prediction(pred, ytest)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
```

## hcluster clustering 

```{r perform kmean}
# sub4 = sub3[,-1099]
# kmeanfit <- kmeans(sub4, 2)
# table((kmeanfit$cluster - 1),sub3$y)

data1 = dist(sub3)
complete_hc <- hclust(data1,method = "complete")
# plot(complete_hc)
single_hc <- hclust(data1,method = "single")
# plot(single_hc)
average_hc <- hclust(data1,method = "average")
# plot(average_hc)
clusters = cutree(complete_hc, k = 2)
table((clusters - 1),sub3$y)

roc2 <- prediction((clusters - 1), sub3$y)
# calculates the ROC curve
perf2 <- performance(roc2,"tpr","fpr")
plot(perf2,colorize=TRUE)
performance(roc2, measure = "auc")@y.values[[1]]
```

# Variable Selection for All Outcomes (random forest?) (est. 2-3 pages. pt.20)

Using Random Forest, select the most important 50 variables, and make predictions based on these variables. 

```{r, include=FALSE}
library(randomForest)
rf.fit = randomForest(Xtrain, ytrain, 
                      ntree=1000, 
                      mtry=20, 
                      nodesize=5, 
                      samplesize=400, 
                      importance=TRUE)
```

```{r, include=FALSE}
pred = predict(rf.fit, Xtest)
confusion_table = table("predicted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

```{r}
impt = rf.fit$importance[order(rf.fit$importance[,3], decreasing=TRUE),][1:50,]
vars = rownames(impt)
```

```{r}
# sub4 is the cleaned dataset with all four response variables
sub4 = subset(sub, select = vars)
sub4 = cbind(sub4, sub[1937:1940])
sub4$PR.Status = as.factor(sub4$PR.Status)
sub4$histological.type = as.factor(sub4$histological.type)
sub4$ER.Status = as.factor(sub4$ER.Status)
sub4$HER2.Final.Status = as.factor(sub4$HER2.Final.Status)
```

```{r}
Xtest = sub4[test_idx, 1:50]
Xtrain = sub4[-test_idx, 1:50]
```

```{r}
ytest = sub4$ER.Status[test_idx]
ytrain = sub4$ER.Status[-test_idx]
svm.fit = svm(ytrain ~., data=Xtrain, 
              type="C-classification", kernel="linear", scale=F, cost=1)
table("predicted" = svm.fit$fitted, "actual" = ytrain)
pred = predict(svm.fit, newdata = Xtest)
table("predicted" = pred, "actual" = ytest)
```

```{r}
library(ROCR)
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr") 
plot(perf, colorize = T)
```

```{r}
rf.fit = randomForest(Xtrain, ytrain, 
                      ntree=500, 
                      mtry=7, 
                      nodesize=10, 
                      samplesize=400, 
                      importance=TRUE)

pred = predict(rf.fit, Xtest)
table("predicted" = pred, "actual" = ytest)
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr") 
plot(perf, colorize = T)
```


