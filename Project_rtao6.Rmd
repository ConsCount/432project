---
title: "Project"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

<style>
body {
text-align: justify}
</style>

```{css, echo=FALSE}
.solution {
background-color: #CCDDFF;
}
```

**Note: have not finished data preprocessing**   

# Project Description (est. 1 page, pt. 5)

# Literature Review(est. 1 page, pt.10)

# Summary Statistics and Data Preprocessing (est. 1 -2 pages, pt.10)

## Data Overview  
The dataset has 705 observations and 1941 features (1936 predictors and 5 outcomes). There are four different kinds of predictors: `rs` (gene expression), `cn` (copy number variations), `mu` (mutations), and `pp` (protein levels). Among them, `rs` and `pp` are continuous variables, and `cn` and `mu` are categorical variables. 

```{r load data, include=FALSE}
brca = read.csv("brca_data_w_subtypes.csv")
```

```{r, include=FALSE}
dim(brca) # 705 rows, 1941 columns
names(brca)[1937:1941] # outcomes 
# 1936 covariates: 860 copy number variations (cn), 249 somatic mutations (mu), 604 gene expressions (rs), and 223 protein levels (pp)
```

```{r, include=FALSE}
brca = brca[,-1937] # discard `vital.status` 
names(brca)[1937:1940]
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="23%"}
hist(brca$rs_CLEC3A, main="Histogram of rs_CLEC3A")
hist(brca$cn_A2ML1, main="Histogram of cn_A2ML1")
hist(brca$mu_ABCA12, main="Histogram of mu_ABCA12")
hist(brca$pp_A.Raf, main="pp_A.Raf")
```

## Remove Missing Values
According to the instruction, we dropped `vital.status`, and we only considered each response variable as a binary variable. Therefore, we treated the observations that had other outcomes as missing values and removed them from our dataset. 

```{r, include=FALSE}
# only use Negative and Positive for PR.status, ER.status, and HER2.Final.Status
# PR.status and ER.status are highly correlated
table(brca$PR.Status) 
table(brca$ER.Status)
table(brca$HER2.Final.Status)
table(brca$histological.type)
```

Then the dataset `sub` had 507 observations and 1940 features. 
```{r}
# sub is the dataset containing no missing values 
sub = brca[(brca$PR.Status == "Positive" | brca$PR.Status == "Negative") & 
           (brca$ER.Status == "Positive" | brca$ER.Status == "Negative") & 
           (brca$HER2.Final.Status == "Positive" | 
            brca$HER2.Final.Status == "Negative"),]
dim(sub)
```

```{r, include=FALSE}
# the input variables have the indices below 
# rs 1:604, cn 605:1464, mu 1465:1713, pp 1714:1936
```

## Deal with Multicollinearity 
One of the noticeable characteristics of the data is its high dimensionality. There are 1936 predictors, almost four times as many as there are observations. Therefore, it is essential to check correlation.   

Since there are four kinds of predictors, it is unlikely that two variables that belong to different kinds would be highly correlated. Also, to reduce the computational cost, we split the data into four subsets: `rs`, `cn`, `mu`, and `pp`, each of which contained only ond kind of predictors.  

Then, we created the correlation matrix for each subset, and extracted variables that are highly-correlated with at least one other variable. Take `rs` as an example. The dataframe `idx` stores all matrix indices of highly-correlated variables and the corresponding correlation coefficients. If the i-th variable is highly-correlated with the j-th variable, then we only need one of them. Thus, we removed all variables with indices `i`. For `rs`, 94 predictors were removed. We applied the same process to the other three subsets. In total, 882 predictors were removed. There are 1059 predictors remained. 

```{r, include=FALSE}
rs = sub[1:604] # the subset that only contains rs
corr = round(cor(rs), 2) # correlation matrix 
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,] # stores correlations that are greater than 0.8 -> multicollinearity 
# dim(idx)
```

```{r}
names(idx) = c("i", "j", "corr")
idx[1:3,]
# remove highly-correlated variables 
rmv = unique(idx[,1])
length(rmv)
rs = rs[,-rmv]
```

```{r, include=FALSE}
cn = sub[605:1464]
corr = round(cor(cn), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)

cn = cn[,-rmv]
```

```{r, include=FALSE}
mu = sub[1465:1713]
corr = round(cor(mu), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)
# there is no multicollinearity within mu, so no variable is removed here 
# mu = mu[,-rmv]
```

```{r, include=FALSE}
pp = sub[1714:1936]
corr = round(cor(pp), 2)
idx = data.frame(NA, NA, NA)
for (i in 1:nrow(corr)) {
  for (j in 1:nrow(corr)) {
    if (abs(corr[i, j]) > 0.8 & i < j) {
      idx[nrow(idx) + 1,] = c(i, j, corr[i, j])
    }
  }
}
idx = idx[-1,]
dim(idx)
names(idx) = c("i", "j", "corr")

rmv = unique(idx[,1])
length(rmv)

pp = pp[,-rmv]
```

## Continuous Predictors 
As mentioned before, `rs` and `pp` are continuous variables, so we should examine if there are any outliers. We first normalized the variable, and stored row and column indices if the data point was three standard deviations away from the variable mean. For the two subsets, `rs` had 100 outliers, and `pp` had no outlier.   

We further looked into `rs` predictors that included outliers, and we found the vast majority of them had a long tail, mostly right and some left. In addition, a number of `rs` predictors that did not contain outliers also had a non-standard distribution. As a result, a log transformation of `rs` predictors would be beneficial.  

```{r, include=FALSE}
n_outlier = 0
row_idx_rs = c()
col_idx_rs = c()
for (i in 1:ncol(rs)) {
  col = rs[,i]
  col = qnorm(rank(col) / (1 + length(col))) # normalize 
  n_outlier = length(col[col < mean(col) - 3 * sd(col) | 
                           col > mean(col) + 3 * sd(col)])
  if (n_outlier > 0) {
    col_idx_rs = c(col_idx_rs, i)
    row_idx_rs = c(row_idx_rs, which(col %in% col[col < mean(col) - 3 * sd(col) | 
                                              col > mean(col) + 3 * sd(col)]))
  }
}
row_idx_rs = unique(row_idx_rs)
length(row_idx_rs)
length(col_idx_rs)
```

```{r, include=FALSE}
n_outlier = 0
row_idx_pp = c()
col_idx_pp = c()

for (i in 1:ncol(pp)) {
  col = pp[,i]
  col = qnorm(rank(col) / (1 + length(col))) # normalize 
  n_outlier = length(col[col < mean(col) - 3 * sd(col) | col > mean(col) + 3 * sd(col)])
  if (n_outlier > 0) {
    col_idx_pp = c(col_idx_pp, i)
    row_idx_pp = c(row_idx_pp, which(col %in% col[col < mean(col) - 3 * sd(col) | 
                                              col > mean(col) + 3 * sd(col)]))
  }
}

row_idx_pp = unique(row_idx_pp)
length(row_idx_pp)
length(col_idx_pp)
```

```{r, include=FALSE}
row_idx = unique(c(row_idx_rs, row_idx_pp))
length(row_idx)
```

```{r, include=FALSE}
# columns with outliers are skewed (mostly right-skewed)
# for (i in col_idx_rs[1:20]) {
#   hist(rs[,i], main = names(rs)[i])
# }
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="30%"}
# outlier example 
hist(rs[,col_idx_rs[1]], 
     main = paste("right-skewed example:", names(rs)[col_idx_rs[1]]))

# no outlier example 
hist(rs[,1], 
     main = paste("right-skewed example (no outlier):", names(rs)[1]))
hist(rs[,9], 
     main = paste("two mode example (no outlier):", names(rs)[9]))
```

```{r, include=FALSE}
# columns without outliers 
# for (i in 1:16) {
#   hist(rs[,i])
# }
```

Unlike `rs`, `pp` variables were distributed quite normally. However, many of the variables would contain outliers without normalization. Therefore, we normalized `pp` variables. 

```{r, include=FALSE}
# log transform rs and normalize pp 
rs_transformed = rs
pp_normalized = pp
for (i in 1:ncol(rs)) {
  rs_transformed[,i] = log(1 + rs[,i])
}

for (i in 1:ncol(pp)) {
  pp_normalized[,i] = qnorm(rank(pp[,i]) / (1 + length(pp[,i])))
}
```

```{r, echo=FALSE, fig.show="hold", fig.align="center", out.width="40%"}
hist(pp[,1], 
     main = paste("Before Normalization", names(pp)[1]))
hist(pp_normalized[,1], 
     main = paste("After Normalization", names(pp_normalized)[1]))
```

## Categorical Predictors 

# models that perform well on high-dimensional data
- SVM 
- Random Forest 
- Lasso (?)
- KNN regression

# PR Status (Modeling, SVM and random Forest) (est. 2-3 pages, pt.20)
```{r}
# cleaned dataset with PR.status as response 
y = as.factor(sub$PR.Status)
y = ifelse(y == "Positive", 1, 0)
# sub2 = cbind(rs, cn, mu, pp, y) 
sub2 = cbind(rs_transformed, cn, mu, pp_normalized, y)
dim(sub2)
```

```{r}
set.seed(651978735) 
n = dim(sub)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size 

Xtest = sub2[test_idx, -ncol(sub2)]
Xtrain = sub2[-test_idx, -ncol(sub2)]

ytest = sub2[test_idx, ncol(sub2)]
ytrain = sub2[-test_idx, ncol(sub2)]
```

```{r}
library(e1071)
svm.fit = svm(ytrain ~., data=Xtrain, 
              type="C-classification", kernel="linear", scale=F, cost=1)
table("fitted" = svm.fit$fitted, "actual" = ytrain) # in-sample confusion matrix 
```

```{r}
pred = predict(svm.fit, newdata = Xtest)
confusion_table = table("fitted" = pred, "actual" = ytest)
confusion_table
# (34 + 69) / (34 + 69 + 14 + 9)
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```

```{r}
library(ROCR)
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr") 
plot(perf, colorize = T)
```

```{r}
library(randomForest)
rf.fit = randomForest(Xtrain, as.factor(ytrain), 
                      ntree=500, 
                      mtry=10, 
                      nodesize=10, 
                      samplesize=400, 
                      importance=TRUE)
```

```{r}
pred = predict(rf.fit, Xtest)
confusion_table = table("fitted" = pred, "actual" = ytest)
confusion_table
(confusion_table[1, 1] + confusion_table[2, 2]) / test_size
```


```{r}
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr") 
plot(perf, colorize = T)
```


# Histological Type (hcluster and knn regression) (est 2-3 pages, pt.20)
```{r some-processing}
y = as.factor(sub$histological.type)
y = ifelse(y == "infiltrating lobular carcinoma", 1, 0)
sub3 = cbind(rs, cn, mu, pp, y) # cleaned dataset with PR.status as response 
dim(sub3)
```

```{r split-train-test-data}
set.seed(651978735) 
n = dim(sub)[1]
test_size = as.integer(0.25 * n)
test_idx = sample(1:n, test_size) # 25% of the sample size 

Xtest = sub3[test_idx, -ncol(sub3)]
Xtrain = sub3[-test_idx, -ncol(sub3)]

ytest = sub3[test_idx, ncol(sub3)]
ytrain = sub3[-test_idx, ncol(sub3)]
```

```{r perform knn-regression}

```

```{r perform kmean}
sub4 = sub3[,-1099]
kmeanfit <- kmeans(sub4, 2)
table((kmeanfit$cluster - 1),sub3$y)
acc = (394 + 2)/ nrow(sub4)
acc
```

# Variable Selection for All Outcomes (random forest?) (est. 2-3 pages. pt.20)

Using Random Forest, select the most important 50 variables, and make predictions based on these variables. 
```{r}
impt = importance(rf.fit)[order(importance(rf.fit)[,3], decreasing=TRUE),][1:50,]
vars = rownames(impt)
```

```{r}
# sub4 is the cleaned dataset with all four response variables
sub4 = subset(sub, select = vars)
sub4 = cbind(sub4, sub[1937:1940])
sub4$PR.Status = as.factor(sub4$PR.Status)
sub4$histological.type = as.factor(sub4$histological.type)
sub4$ER.Status = as.factor(sub4$ER.Status)
sub4$HER2.Final.Status = as.factor(sub4$HER2.Final.Status)
```

```{r}
Xtest = sub4[test_idx, 1:50]
Xtrain = sub4[-test_idx, 1:50]
```

```{r}
ytest = sub4$ER.Status[test_idx]
ytrain = sub4$ER.Status[-test_idx]
svm.fit = svm(ytrain ~., data=Xtrain, 
              type="C-classification", kernel="linear", scale=F, cost=1)
table("fitted" = svm.fit$fitted, "actual" = ytrain)
pred = predict(svm.fit, newdata = Xtest)
table("fitted" = pred, "actual" = ytest)
```


```{r}
# library(ROCR)
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr") 
plot(perf, colorize = T)
```

```{r}
rf.fit = randomForest(Xtrain, ytrain, 
                      ntree=500, 
                      mtry=7, 
                      nodesize=10, 
                      samplesize=400, 
                      importance=TRUE)

pred = predict(rf.fit, Xtest)
table("fitted" = pred, "actual" = ytest)
roc = prediction(as.numeric(pred), ytest)
performance(roc, measure = "auc")@y.values[[1]]

perf = performance(roc, "tpr", "fpr") 
plot(perf, colorize = T)
```


